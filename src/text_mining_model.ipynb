{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "dataset = pd.read_excel(\"../Data/Womens Clothing Reviews Data New.xlsx\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download necessary resources for SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your dataset into the variable 'dataset'\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate sentiment scores using SentimentIntensityAnalyzer\n",
    "def get_sentiment_score(row):\n",
    "    sentiment_score = analyzer.polarity_scores(row['Merged_Review'])\n",
    "    return sentiment_score['compound']\n",
    "\n",
    "# Apply the sentiment analysis function to each row of the DataFrame\n",
    "dataset['sentiment_score'] = dataset.apply(get_sentiment_score, axis=1)\n",
    "\n",
    "# Function to get sentiment category based on the compound score\n",
    "def get_sentiment_category(compound_score):\n",
    "    if compound_score > 0.1:\n",
    "        return 'Positive'\n",
    "    elif compound_score < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the sentiment analysis function to each row of the DataFrame\n",
    "dataset['sentiment_category'] = dataset['sentiment_score'].apply(get_sentiment_category)\n",
    "\n",
    "# Display the DataFrame with the added 'sentiment_category' column\n",
    "print(dataset[['Merged_Review', 'sentiment_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total positive, negative, and neutral sentiments using value_counts()\n",
    "sentiment_counts = dataset['sentiment_category'].value_counts()\n",
    "\n",
    "# Display the total positive, negative, and neutral sentiments\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "\n",
    "# # Assuming sentiment_counts is a DataFrame containing the sentiment counts\n",
    "# # Create a bar plot using Plotly Express to visualize the distribution of sentiments\n",
    "# fig_bar = px.bar(sentiment_counts, x='sentiment_category', y='count', color='sentiment_category',\n",
    "#                  labels={'sentiment_category': 'Sentiment', 'count': 'Count'},\n",
    "#                  title='Sentiment Distribution in the Dataset')\n",
    "\n",
    "# # Show the bar plot\n",
    "# fig_bar.show()\n",
    "\n",
    "# # Create a pie chart using Plotly Express to visualize the distribution of sentiments\n",
    "# fig_pie = px.pie(sentiment_counts, values='count', names='sentiment_category',\n",
    "#                  title='Sentiment Distribution in the Dataset')\n",
    "\n",
    "# # Show the pie chart\n",
    "# fig_pie.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec \n",
    "# import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# # Calculate the total positive, negative, and neutral sentiments using value_counts()\n",
    "# sentiment_counts = dataset['sentiment_category'].value_counts().reset_index()\n",
    "\n",
    "# # Create a bar plot using Plotly Express to visualize the distribution of sentiments\n",
    "# fig_bar = px.bar(sentiment_counts, x='index', y='sentiment_category', color='index',\n",
    "#                  labels={'index': 'Sentiment', 'sentiment_category': 'Count'},\n",
    "#                  title='Sentiment Distribution in the Dataset')\n",
    "\n",
    "# # Create a pie chart using Plotly Express to visualize the distribution of sentiments\n",
    "# fig_pie = px.pie(sentiment_counts, values='sentiment_category', names='index',\n",
    "#                  title='Sentiment Distribution in the Dataset')\n",
    "\n",
    "# # Display both the bar plot and pie chart side by side\n",
    "# fig_bar.show()\n",
    "# fig_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(dataset.sentiment_category, dataset.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Divide the data into three groups on the basis of sentiments like positive, negative and neutral \"\"\"\n",
    "dataset_neg = dataset[(dataset.sentiment_category=='Negative')]\n",
    "dataset_pos = dataset[(dataset.sentiment_category=='Positive')]\n",
    "dataset_neu = dataset[(dataset.sentiment_category=='Neutral')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plit the data into train & Test where y variable is Rating \"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "## X-variable is Merged_Review and y-variable is Rating\n",
    "# define X and y\n",
    "X = dataset.Merged_Review\n",
    "y = dataset.Rating\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Split the data into train & Test for positive sentiments and negative sentiments \"\"\"\n",
    "# create a new DataFrame that only contains the 5 Rating and 1-Rating reviews\n",
    "# define X and y\n",
    "X2 = dataset_pos.Merged_Review\n",
    "y2 = dataset_pos.Rating\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=1)\n",
    "print(X2_train.shape)\n",
    "print(X2_test.shape)\n",
    "print(y2_train.shape)\n",
    "print(y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X1 = dataset_neg.Merged_Review\n",
    "y1 = dataset_neg.Rating\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, random_state=1)\n",
    "print(X1_train.shape)\n",
    "print(X1_test.shape)\n",
    "print(y1_train.shape)\n",
    "print(y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Creating user defined functions for clean the text and pre-process the data \"\"\"\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "def clean_text(Merged_Review):\n",
    "    Merged_Review = Merged_Review.lower()\n",
    "    Merged_Review = Merged_Review.strip()\n",
    "    Merged_Review = re.sub(r' +', ' ', Merged_Review)\n",
    "    Merged_Review = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?*&£%€¦_><‘|,'0-9]\", \"\", Merged_Review)\n",
    "    Merged_Review = Merged_Review.replace('wat', 'what').replace('txts', 'texts').replace('vry', 'very').replace('gud', 'good').replace('nyt', 'night').replace('msg', 'message')\n",
    "    return Merged_Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = list(set(nltk.corpus.stopwords.words('english')))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_process(Merged_Review):\n",
    "    Merged_Review = Merged_Review.str.replace('/', '')  # Replacing the / with none\n",
    "    Merged_Review = Merged_Review.apply(lambda x: \" \".join(word for word in word_tokenize(x) if word not in stop))  # Removing stop words\n",
    "    Merged_Review = Merged_Review.apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in word_tokenize(x)))  # Lemmatization\n",
    "    return Merged_Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: clean_text(x))\n",
    "X_test = X_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pre_process(X_train)\n",
    "X_test=pre_process(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLean the text and pre-process the data for positive sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = X2_train.apply(lambda x: clean_text(x))\n",
    "X2_test = X2_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train=pre_process(X2_train)\n",
    "X2_test=pre_process(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLean the text and pre-process the data for negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = X1_train.apply(lambda x: clean_text(x))\n",
    "X1_test = X1_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train=pre_process(X1_train)\n",
    "X1_test=pre_process(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization (Count, Tfidf) for positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "#Train\n",
    "count_vect2 = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), \n",
    "                             min_df=5, \n",
    "                             encoding='latin-1' ,\n",
    "                             max_features=800)\n",
    "xtrain2_count = count_vect2.fit_transform(X2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain2_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization (Count, Tfidf) for negative sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect1 = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), \n",
    "                             min_df=5, \n",
    "                             encoding='latin-1' ,\n",
    "                             max_features=800)\n",
    "xtrain1_count = count_vect1.fit_transform(X1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the document term metrics for positive sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=xtrain2_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vect2.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm1=pd.DataFrame(dtm, columns = count_vect2.get_feature_names_out())\n",
    "dtm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm1.apply(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the document term metrics for negative sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm4=xtrain1_count.toarray()\n",
    "print(count_vect1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm5=pd.DataFrame(dtm4, columns = count_vect1.get_feature_names_out())\n",
    "dtm5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm5.apply(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequencies for positive sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = pd.DataFrame(dtm1.apply(sum).head(40), columns=['freq'])\n",
    "word_freq.sort_values('freq', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_dictionary = dict(dtm1.apply(sum))\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "\n",
    "# # Assuming word_freq is a DataFrame containing the word frequencies\n",
    "# # Sort the DataFrame by frequency in descending order\n",
    "# word_freq.sort_values('freq', ascending=False, inplace=True)\n",
    "\n",
    "# # Reset the index (and drop the existing 'level_0' column)\n",
    "# word_freq.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Create the bar graph using Plotly Express\n",
    "# fig = px.bar(word_freq, x='index', y='freq', color='index', color_discrete_sequence=px.colors.qualitative.Pastel1)\n",
    "\n",
    "# # Customize the layout\n",
    "# fig.update_layout(\n",
    "#     title='Word Frequency',\n",
    "#     xaxis_title='Words',\n",
    "#     yaxis_title='Frequency',\n",
    "#     showlegend=False  # Hide the legend\n",
    "# )\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequencies for negative sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq1 = pd.DataFrame(dtm5.apply(sum).head(40), columns=['freq'])\n",
    "word_freq1.sort_values('freq', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_dictionary1 = dict(dtm5.apply(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_dictionary1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making word_clouds for postive sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Create a custom color map\n",
    "def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * random.random())\n",
    "    s = int(100.0 * random.random())\n",
    "    l = int(50.0 + 20.0 * random.random())\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
    "\n",
    "# Calculate the desired aspect ratio (width:height)\n",
    "aspect_ratio = 1  # Since the desired figsize is 12x12, the aspect ratio is 1 (square)\n",
    "\n",
    "# Calculate the width and height based on the aspect ratio and desired figsize\n",
    "figsize = (12, 12)  # Desired figsize\n",
    "width = int(figsize[0] * 100)  # Convert to inches and then to pixels\n",
    "height = int(width / aspect_ratio)\n",
    "\n",
    "# Create a WordCloud object with custom parameters\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=STOPWORDS,\n",
    "    color_func=random_color_func,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    max_words=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate the word cloud from word_freq_dictionary\n",
    "wordcloud = wordcloud.generate_from_frequencies(word_freq_dictionary)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=figsize)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making word_clouds for negative sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Create a custom color map\n",
    "def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * random.random())\n",
    "    s = int(100.0 * random.random())\n",
    "    l = int(50.0 + 20.0 * random.random())\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
    "\n",
    "# Calculate the desired aspect ratio (width:height)\n",
    "aspect_ratio = 1  # Since the desired figsize is 12x12, the aspect ratio is 1 (square)\n",
    "\n",
    "# Calculate the width and height based on the aspect ratio and desired figsize\n",
    "figsize = (12, 12)  # Desired figsize\n",
    "width = int(figsize[0] * 100)  # Convert to inches and then to pixels\n",
    "height = int(width / aspect_ratio)\n",
    "\n",
    "# Create a WordCloud object with custom parameters\n",
    "wordcloud1 = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=STOPWORDS,\n",
    "    color_func=random_color_func,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    max_words=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate the word cloud from word_freq_dictionary\n",
    "wordcloud1 = wordcloud1.generate_from_frequencies(word_freq_dictionary1)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=figsize)\n",
    "plt.imshow(wordcloud1, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Understand sentiment among the customers on the different categories, sub categories,products by location and age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['age_group'] = pd.cut(x= dataset.Customer_Age,bins=[20, 29, 39, 49,59 ,69,79,89 ,99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(['Location', 'age_group','Category','SubCategory1','SubCategory2',\"sentiment_category\" ]).agg({'sentiment_category': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "d. Perform predictive analytics to understand the drivers of customers who are \n",
    "recommending the products. \n",
    "\n",
    "Regression model\n",
    "Vectorization (count, tfidf) for both train & test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Split the data in X and Y ( Recommend_Flag) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X-variable is Review_text and y-variable is Rating\n",
    "# define X and y\n",
    "X4 = dataset.Merged_Review\n",
    "y4 = dataset.Recommend_Flag\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, random_state=1)\n",
    "print(X4_train.shape)\n",
    "print(X4_test.shape)\n",
    "print(y4_train.shape)\n",
    "print(y4_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a model using X_train (Merged_Review) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train = X4_train.apply(lambda x: clean_text(x))\n",
    "X4_test = X4_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "count_vect = CountVectorizer(analyzer='word', \n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), \n",
    "                             min_df=5, \n",
    "                             lowercase=True,\n",
    "                             encoding='latin-1', \n",
    "                             max_features=100)\n",
    "X_train_count4 = count_vect.fit_transform(X4_train)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             ngram_range=(1, 1 ), \n",
    "                             min_df=5, \n",
    "                             encoding='latin-1', \n",
    "                             lowercase=True,\n",
    "                             max_features=100)\n",
    "X_train_tfidf4 = tfidf_vect.fit_transform(X4_train)\n",
    "\n",
    "# Test\n",
    "X_test_count4 = count_vect.transform(X4_test)\n",
    "X_test_tfidf4 = tfidf_vect.transform(X4_test)\n",
    "\n",
    "dtm_count = pd.DataFrame(X_train_count4.toarray(), columns=count_vect.get_feature_names_out())\n",
    "dtm_tfidf = pd.DataFrame(X_train_tfidf4.toarray(), columns=tfidf_vect.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Adding Features to a Document-Term Matrix\n",
    "\n",
    "Dummy Creation\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An utility function to create dummy variable\n",
    "def create_dummies(dataset, colname):\n",
    "    col_dummies = pd.get_dummies(dataset[colname], prefix = colname, drop_first = True)\n",
    "    dataset = pd.concat([dataset, col_dummies], axis = 1)\n",
    "    dataset.drop(colname, axis = 1, inplace = True )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagory_varables = dataset[['Category', 'SubCategory1', 'SubCategory2', 'sentiment_category','Location', 'Channel']]\n",
    "\n",
    "# for c_feature in categorical_features\n",
    "for c_feature in ['Category', 'SubCategory1', 'SubCategory2', 'sentiment_category','Location', 'Channel']:\n",
    "    catagory_varables[c_feature] = catagory_varables[c_feature].astype('category')\n",
    "    catagory_varables = create_dummies(catagory_varables, c_feature)\n",
    " \n",
    "\n",
    "catagory_varables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new =dataset.loc[:, [ 'Merged_Review', 'Customer_Age', 'Rating', 'sentiment_score','Recommend_Flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([ dataset_new,catagory_varables], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and y\n",
    "feature_cols = ['Merged_Review', 'Customer_Age', 'Rating', 'sentiment_score',\n",
    "       'Recommend_Flag', 'Category_General Petite', 'Category_Initmates',\n",
    "       'SubCategory1_Dresses', 'SubCategory1_Intimate', 'SubCategory1_Jackets',\n",
    "       'SubCategory1_Tops', 'SubCategory1_Trend',\n",
    "       'SubCategory2_Casual bottoms', 'SubCategory2_Chemises',\n",
    "       'SubCategory2_Dresses', 'SubCategory2_Fine gauge',\n",
    "       'SubCategory2_Intimates', 'SubCategory2_Jackets', 'SubCategory2_Jeans',\n",
    "       'SubCategory2_Knits', 'SubCategory2_Layering', 'SubCategory2_Legwear',\n",
    "       'SubCategory2_Lounge', 'SubCategory2_Outerwear', 'SubCategory2_Pants',\n",
    "       'SubCategory2_Shorts', 'SubCategory2_Skirts', 'SubCategory2_Sleep',\n",
    "       'SubCategory2_Sweaters', 'SubCategory2_Swim', 'SubCategory2_Trend',\n",
    "       'sentiment_category_Neutral', 'sentiment_category_Positive',\n",
    "       'Location_Chennai', 'Location_Gurgaon', 'Location_Mumbai',\n",
    "       'Channel_Web']\n",
    "X = data[feature_cols]\n",
    "y = data.Recommend_Flag\n",
    "\n",
    "#split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Merged_Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use  TfidfVectorizer with Merged_Review column only\n",
    "vect = TfidfVectorizer(lowercase=True, stop_words='english', max_features=100, min_df=5, ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train.Merged_Review)\n",
    "X_test_dtm = vect.transform(X_test.Merged_Review)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('Merged_Review', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer with Merged_Review column only\n",
    "vect = CountVectorizer(lowercase=True, stop_words='english', max_features=100, min_df=5, ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train.Merged_Review)\n",
    "X_test_dtm = vect.transform(X_test.Merged_Review)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('Merged_Review', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sparse.csr_matrix(X_train.drop('Merged_Review', axis=1).astype(float))\n",
    "extra.shape\n",
    "\n",
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape\n",
    "\n",
    "# repeat for testing set\n",
    "extra = sparse.csr_matrix(X_test.drop('Merged_Review', axis=1).astype(float))\n",
    "X_test_dtm_extra = sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the score for validation\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "tr_pred=logreg.predict(X_train_dtm)\n",
    "y_pred = logreg.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "trprecision,trrecall,trfscore,trsupport=score(y_train,tr_pred)\n",
    "tracc=accuracy_score(y_train,tr_pred)\n",
    "precision,recall,fscore,support=score(y_test,y_pred)\n",
    "acc=accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training\n",
    "\n",
    "print('Precision : ',trprecision)\n",
    "print('\\nRecall : ',trrecall)\n",
    "print('\\nF-Score :',trfscore)\n",
    "print('\\nAccuracy : ',tracc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing\n",
    "\n",
    "print('Precision : ',precision)\n",
    "print('\\nRecall : ',recall)\n",
    "print('\\nF-Score :',fscore)\n",
    "print('\\nAccuracy : ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model\n",
    "import pickle\n",
    "Pkl_Filename = \"Pickle_LR_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(logreg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Reloaded Model to \n",
    "# Calculate the accuracy score and predict target values\n",
    "\n",
    "# Calculate the Score \n",
    "score = Pickled_LR_Model.score(X_test_dtm, y_test)  \n",
    "# Print the Score\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  \n",
    "\n",
    "# Predict the Labels using the reloaded Model\n",
    "Ypredict = Pickled_LR_Model.predict(X_test_dtm)  \n",
    "Ypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    So, Above we made one logistic model where Y variable is recommend_flag and X variable \n",
    "    is review_text. The model gives accuracy of 85.73% in train and test. The difference between \n",
    "    train and test accuracy is less so, we can say the model is good to use. The main key \n",
    "    drivers who are responsible for recommending the product are as above.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and y\n",
    "feature_cols = ['Merged_Review', 'Customer_Age', 'Rating', 'sentiment_score',\n",
    "       'Recommend_Flag', 'Category_General Petite', 'Category_Initmates',\n",
    "       'SubCategory1_Dresses', 'SubCategory1_Intimate', 'SubCategory1_Jackets',\n",
    "       'SubCategory1_Tops', 'SubCategory1_Trend',\n",
    "       'SubCategory2_Casual bottoms', 'SubCategory2_Chemises',\n",
    "       'SubCategory2_Dresses', 'SubCategory2_Fine gauge',\n",
    "       'SubCategory2_Intimates', 'SubCategory2_Jackets', 'SubCategory2_Jeans',\n",
    "       'SubCategory2_Knits', 'SubCategory2_Layering', 'SubCategory2_Legwear',\n",
    "       'SubCategory2_Lounge', 'SubCategory2_Outerwear', 'SubCategory2_Pants',\n",
    "       'SubCategory2_Shorts', 'SubCategory2_Skirts', 'SubCategory2_Sleep',\n",
    "       'SubCategory2_Sweaters', 'SubCategory2_Swim', 'SubCategory2_Trend',\n",
    "       'sentiment_category_Neutral', 'sentiment_category_Positive',\n",
    "       'Location_Chennai', 'Location_Gurgaon', 'Location_Mumbai',\n",
    "       'Channel_Web']\n",
    "X = data[feature_cols]\n",
    "y = data.Rating\n",
    "\n",
    "#split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TfidfVectorizer with Merged_Review column only\n",
    "vect = TfidfVectorizer(lowercase=True, stop_words='english', max_features=100, min_df=5, ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train.Merged_Review)\n",
    "X_test_dtm = vect.transform(X_test.Merged_Review)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('Merged_Review', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg2 = LogisticRegression(C=1e9)\n",
    "logreg2.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg2.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model3=KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
    "model3.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the score for validation\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "tr_pred=logreg2.predict(X_train_dtm)\n",
    "y_pred = logreg2.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "trprecision,trrecall,trfscore,trsupport=score(y_train,tr_pred)\n",
    "tracc=accuracy_score(y_train,tr_pred)\n",
    "precision,recall,fscore,support=score(y_test,y_pred)\n",
    "acc=accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training\n",
    "\n",
    "print('Precision : ',trprecision)\n",
    "print('\\nRecall : ',trrecall)\n",
    "print('\\nF-Score :',trfscore)\n",
    "print('\\nAccuracy : ',tracc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing\n",
    "\n",
    "print('Precision : ',precision)\n",
    "print('\\nRecall : ',recall)\n",
    "print('\\nF-Score :',fscore)\n",
    "print('\\nAccuracy : ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above model is useful where X variable is Merged_Review and Y variable is Rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create user defined function for train Classification the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,  valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes (With only review_text in X-vribles)\n",
    "#Naive Bayes on Count Vectors and TF-IDF\n",
    "from sklearn import naive_bayes\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_dtm, y_train, X_test_dtm, y_test)\n",
    "print(\"NB  for L1, TFIDF Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "#Logistic Regression on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_dtm, y_train, X_test_dtm, y_test)\n",
    "print(\"LR  for L1, tfidf Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVC\n",
    "#Linear SVC on Count Vectors and TF-IDF\n",
    "from sklearn import svm\n",
    "\n",
    "accuracy_L1 = train_model(svm.LinearSVC(), X_train_dtm, y_train, X_test_dtm, y_test)\n",
    "print(\"SVC  for L1, Count Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hence from above logistic regression gives best result.save the model as pickle object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Create topics and understand themes behind the topics by performing TOPIC MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [doc.split() for doc in X_train]\n",
    "X_train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Gensim dictionary from tokenized documents\n",
    "dictionary = corpora.Dictionary(X_train_tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Document-Term Matrix (DTM) using Gensim's doc2bow function\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in X_train_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary,passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.show_topics(formatted=False, num_words=20)\n",
    "    \n",
    "for t in range(len(topics)):\n",
    "    print(\"\\nTopic {}, top {} words:\".format(t+1,30))\n",
    "    print(\" \".join([w[0] for w in topics[t][1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
